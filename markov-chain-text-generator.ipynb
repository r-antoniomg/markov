{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov chain text generator\n",
    "\n",
    "This Markov chain model takes input in the form of .txt files, and uses prompts to generate new text.\n",
    "\n",
    "This Jupyter notebook goes step by step through the process of creating a text generating application.\n",
    "This notebook is intended for beginners without a strong coding background.\n",
    "As such, most of the explanation has to do with basic aspects of the code.\n",
    "The notebook doesn't go into much detail about all the code, especially the more complex functions that make up the text generator.\n",
    "\n",
    "The code was adapted from from Luciano (StrikingLoo's) [ASOIAF-Markov repository](https://github.com/StrikingLoo/ASOIAF-Markov).\n",
    "He also wrote an article called ['Markov Chains: How to Train Text Generation to Write Like George R. R. Martin'](https://www.kdnuggets.com/2019/11/markov-chains-train-text-generation.html), where he goes into more detail around what Markov chains are and how they work in the context of his text generator.\n",
    "\n",
    "Another great resource for learning about different methods of machine learning (including Markov chains) is the book [You look like a thing and I love you](https://www.janelleshane.com/book-you-look-like-a-thing) by Janelle Shane.\n",
    "The book is a good introduction to Artificial Intelligence full of humour and meant for a broad audience (without assumptions about previous coding skills)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 : get libraries ready\n",
    "\n",
    "The following code may be required if any of the libraries that we need to import are not installed.\n",
    "'Libraries' refer to code that someone else has written and made available for reuse.\n",
    "We can simply add that code into our project by 'importing' the library, rather than having to write all that code from scratch.\n",
    "By default, in this notebook the code has been commented out (the # symbol before the code means that the computer will skip that line of code).\n",
    "In order to run the code, if needed, we need to remove the # symbol and run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pandas\n",
    "# !pip install seaborn\n",
    "# !pip install numpy\n",
    "# !pip install glob\n",
    "# !pip install scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code will import the libraries that we will use to run this program.\n",
    "\n",
    "When you run the code below, you may get an error if you are missing a specific library.\n",
    "If that is the case, you can install the missing library by running the code in the cell above.\n",
    "Once you have installed the missing library (or libraries), you can run the cell below again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import glob\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 : get corpus data ready\n",
    "\n",
    "The following code indicates the path to the corpus files.\n",
    "It assumes that the sub-directory (folder) called 'documents' is in the same directory as the Jupyter notebook.\n",
    "\n",
    "The code also indicates that, within the 'documents' directory, we are interested only in text files (anything that has a .txt extension)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = glob.glob('documents/*.txt')\n",
    "\n",
    "# If your documents are stored in a different directory on your computer, you will need to write the full path to retrieve the files as in the following example:\n",
    "#\n",
    "# file_names = glob.glob('C:/Users/example/Desktop/documents/*.txt')\n",
    "#\n",
    "# If you need to use this option, you must also comment-out the first line of code in this block, and un-comment the line that has the full path."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code will tell us how many text files (files with a .txt extension) are in the corpus directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(file_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 : parse the text from the corpus files\n",
    "\n",
    "The following code takes the text in our corpus files and parses it out into sentences, using the period as the delimiter criteria.\n",
    "It does additional cleanup of the text by removing line breaks and tabs from the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences(file_name):\n",
    "    with open(file_name, 'r', encoding='utf-8') as f:\n",
    "        return f.read().split('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_LENGTH = 15\n",
    "sentences = []\n",
    "for file_name in file_names:\n",
    "    sentences+=get_sentences(file_name)\n",
    "\n",
    "sentences = [sentence.replace('\\n','') for sentence in sentences]\n",
    "sentences = [sentence.replace('\\t','') for sentence in sentences]\n",
    "sentences = [sentence for sentence in sentences if len(sentence)>MIN_LENGTH]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = [len(sentence) for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = pd.Series(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths.quantile(.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths.describe()\n",
    "# 14228 our of 18k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code returns the parsed-out sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 : load whole corpus\n",
    "\n",
    "The following code iterates through all the files in the corpus and performs basic cleanup tasks.\n",
    "A few lines of code return basic data about our corpus, such as total number of words, and total number of unique words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \"\"\n",
    "for file_name in file_names:\n",
    "    with open(file_name, 'r', encoding='utf-8') as f:\n",
    "            corpus+=f.read()\n",
    "corpus = corpus.replace('\\n',' ')\n",
    "corpus = corpus.replace('\\t',' ')\n",
    "corpus = corpus.replace('“', ' \" ')\n",
    "corpus = corpus.replace('”', ' \" ')\n",
    "for spaced in ['.','-',',','!','?','(','—',')']:\n",
    "    corpus = corpus.replace(spaced, ' {0} '.format(spaced))\n",
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code outputs a sample of words from the corpus that we built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus[1000:1500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code splits the corpus into individual words, then counts the number of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_words = corpus.split(' ')\n",
    "corpus_words= [word for word in corpus_words if word != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(corpus_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code outputs all the words that are part of the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(corpus_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code indicates the total number of unique words in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distinct_words = list(set(corpus_words))\n",
    "word_idx_dict = {word: i for i, word in enumerate(distinct_words)}\n",
    "distinct_words_count = len(list(set(corpus_words)))\n",
    "distinct_words_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 : build matrix for Markov chain\n",
    "\n",
    "This code will create the matrix that serves as the basis for generating new text, by calculating the most likely co-occurrences of words based on the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_word_matrix = np.zeros([distinct_words_count,distinct_words_count])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_idx_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, word in enumerate(corpus_words[:-1]):\n",
    "    first_word_idx = word_idx_dict[word]\n",
    "    next_word_idx = word_idx_dict[corpus_words[i+1]]\n",
    "    next_word_matrix[first_word_idx][next_word_idx] +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_likely_word_after(aWord):\n",
    "    most_likely = next_word_matrix[word_idx_dict[aWord]].argmax()\n",
    "    return distinct_words[most_likely]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6 : test the code to obtain sample sentences\n",
    "\n",
    "The following code prepares the program to print some sample sentences:\n",
    "\n",
    "By default, the sample sentences will be 15 words long (the value of 'length').\n",
    "You may replace the length value to a different integer in order to get longer or shorter output sentences.\n",
    "\n",
    "Default length = 15:\n",
    "\n",
    "`def naive_chain(seed, length=15)`\n",
    "\n",
    "Modified example, length = 8:\n",
    "\n",
    "`def naive_chain(seed, length=8)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_chain(seed, length=15):\n",
    "    current_word = seed\n",
    "    sentence = seed\n",
    "\n",
    "    for _ in range(length):\n",
    "        sentence+=' '\n",
    "        next_word = most_likely_word_after(current_word)\n",
    "        sentence+=next_word\n",
    "        current_word = next_word\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code will print sample sentences.\n",
    "\n",
    "Given an initial prompt word (seed), the program calculates the most likely word to occur after, and iterates through this process until it reaches the length of the sentence that was defined above.\n",
    "\n",
    "In the code below, you can alter the seed value(s) to see what happens if you use other words as the prompt.\n",
    "\n",
    "Example:\n",
    "\n",
    "Default = `print(naive_chain('the'))`\n",
    "\n",
    "Modified = `print(naive_chain('when'))`\n",
    "\n",
    "The code may crash if the prompt that you use does not match any of the words that form part of the original corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(naive_chain('the'))\n",
    "print(naive_chain('I'))\n",
    "print(naive_chain('he'))\n",
    "print(naive_chain('she'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from random import random \n",
    "\n",
    "def weighted_choice(objects, weights):\n",
    "    \"\"\" returns randomly an element from the sequence of 'objects', \n",
    "        the likelihood of the objects is weighted according \n",
    "        to the sequence of 'weights', i.e. percentages.\"\"\"\n",
    "\n",
    "    weights = np.array(weights, dtype=np.float64)\n",
    "    sum_of_weights = weights.sum()\n",
    "    # standardization:\n",
    "    np.multiply(weights, 1 / sum_of_weights, weights)\n",
    "    weights = weights.cumsum()\n",
    "    x = random()\n",
    "    for i in range(len(weights)):\n",
    "        if x < weights[i]:\n",
    "            return objects[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import choice\n",
    "\n",
    "def sample_next_word_after(aWord, alpha = 0):\n",
    "    next_word_vector = next_word_matrix[word_idx_dict[aWord]] + alpha\n",
    "    likelihoods = next_word_vector/next_word_vector.sum()\n",
    "    return weighted_choice(distinct_words, likelihoods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_next_word_after('the')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_chain(seed, length=15):\n",
    "    current_word = seed\n",
    "    sentence = seed\n",
    "\n",
    "    for _ in range(length):\n",
    "        sentence+=' '\n",
    "        next_word = sample_next_word_after(current_word)\n",
    "        sentence+=next_word\n",
    "        current_word = next_word\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stochastic_chain('table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'John W . I had feasted them mutton , \" You did not even the Eyrie'\n",
    "'the Seven in front of whitefish in a huge blazes burning flesh . I had been'\n",
    "'a squire , slain , they thought . \" He bathed in his head . The'\n",
    "'Bran said Melisandre had been in fear I’ve done . \" It must needs you will'\n",
    "'Melisandre would have feared he’d squired for something else I put his place of Ser Meryn'\n",
    "'Daenerys is dead cat - TOOTH , AT THE GREAT , Asha , which fills our'\n",
    "'Daenerys Targaryen after Melara had worn rich grey sheep to encircle Stannis . \" The deep'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "sets_of_k_words = [ ' '.join(corpus_words[i:i+k]) for i, _ in enumerate(corpus_words[:-k]) ]\n",
    "\n",
    "print([len(list(set(sets_of_k_words))),\n",
    "       len(sets_of_k_words)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import dok_matrix\n",
    "\n",
    "sets_count = len(list(set(sets_of_k_words)))\n",
    "next_after_k_words_matrix = dok_matrix((sets_count, len(distinct_words)))\n",
    "print(next_after_k_words_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "distinct_sets_of_k_words = list(set(sets_of_k_words))\n",
    "k_words_idx_dict = {word: i for i, word in enumerate(distinct_sets_of_k_words)}\n",
    "distinct_k_words_count = len(list(set(sets_of_k_words)))\n",
    "print(len(sets_of_k_words))\n",
    "for i, word in enumerate(sets_of_k_words[:-k]):\n",
    "    if i % 50000 == 0:\n",
    "        print(i)\n",
    "    word_sequence_idx = k_words_idx_dict[word]\n",
    "    next_word_idx = word_idx_dict[corpus_words[i+k]]\n",
    "    next_after_k_words_matrix[word_sequence_idx, next_word_idx] +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_chain(seed, chain_length=15, seed_length=2):\n",
    "    current_words = seed.split(' ')\n",
    "    if len(current_words) != seed_length:\n",
    "        raise ValueError(f'wrong number of words, expected {seed_length}')\n",
    "    sentence = seed\n",
    "\n",
    "    for _ in range(chain_length):\n",
    "        sentence+=' '\n",
    "        next_word = sample_next_word_after_sequence(' '.join(current_words))\n",
    "        sentence+=next_word\n",
    "        current_words = current_words[1:]+[next_word]\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import choice\n",
    "\n",
    "def sample_next_word_after_sequence(word_sequence, alpha = 0):\n",
    "    next_word_vector = next_after_k_words_matrix[k_words_idx_dict[word_sequence]] + alpha\n",
    "    likelihoods = next_word_vector/next_word_vector.sum()\n",
    "    return weighted_choice(distinct_words, likelihoods.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stochastic_chain('the world')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stochastic_chain('Jon Snow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stochastic_chain('Eddard Stark')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stochastic_chain('The game')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stochastic_chain('The game')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stochastic_chain('I have')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stochastic_chain('heard the')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stochastic_chain('that made them look like', 15, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distinct_sets_of_k_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'Maybe we should all do the same , Jon reflected glumly . He made himself eat , hungry or no'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
